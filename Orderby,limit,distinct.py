# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/108yaaEbxOb-xLlOZR5tylwLO3g-E4pUL
"""

!wget -q https://github.com/saiadityaus1/test1/raw/main/df.csv -O df.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/df1.csv -O df1.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/dt.txt -O dt.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file1.txt -O file1.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file2.txt -O file2.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file3.txt -O file3.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file4.json -O file4.json
!wget -q https://github.com/saiadityaus1/test1/raw/main/file5.parquet -O file5.parquet
!wget -q https://github.com/saiadityaus1/test1/raw/main/file6 -O file6
!wget -q https://github.com/saiadityaus1/test1/raw/main/prod.csv -O prod.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/state.txt -O state.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/usdata.csv -O usdata.csv
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar xf spark-3.5.0-bin-hadoop3.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"
import findspark
findspark.init()
import sys
import os
python_path = sys.executable
os.environ['PYSPARK_PYTHON'] = python_path
from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession
conf = SparkConf().setAppName("pyspark").setMaster("local[*]").set("spark.driver.host","localhost").set("spark.driver.allowMultipleContexts","true")
sc = SparkContext(conf=conf)
spark = SparkSession.builder.config("spark.driver.allowMultipleContexts","true").getOrCreate()
print("DONE==== START YOUR WORKðŸ‘‡ ")

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("InterviewData").getOrCreate()

# Employees DataFrame
employees_data = [
    (1, 'Alice', 'Sales', 60000),
    (2, 'Bob', 'HR', 50000),
    (3, 'Charlie', 'Sales', 75000),
    (4, 'David', 'Engineering', 80000),
    (5, 'Eva', 'Engineering', 70000),
    (6, 'Frank', 'HR', 48000),
    (7, 'Grace', 'Marketing', 62000),
    (8, 'Heidi', 'Marketing', 58000)
]
employees_df = spark.createDataFrame(employees_data, ["emp_id", "emp_name", "department", "salary"])

# Customers DataFrame
customers_data = [
    (1, 'John', 'Delhi'),
    (2, 'Jane', 'Mumbai'),
    (3, 'Alice', 'Delhi'),
    (4, 'Bob', 'Bangalore'),
    (5, 'Alice', 'Chennai'),
    (6, 'Tom', 'Mumbai')
]
customers_df = spark.createDataFrame(customers_data, ["customer_id", "name", "city"])

# Sales DataFrame
sales_data = [
    (101, 'Laptop', 15, '2024-12-01'),
    (102, 'Tablet', 25, '2024-12-05'),
    (103, 'Phone', 40, '2024-12-06'),
    (104, 'Monitor', 10, '2024-12-03'),
    (105, 'Keyboard', 5, '2024-12-07')
]
sales_df = spark.createDataFrame(sales_data, ["product_id", "product_name", "quantity_sold", "sale_date"])


#Product DataFrame
products_data = [
    (201, 'Laptop', 85000),
    (202, 'Tablet', 30000),
    (203, 'Phone', 50000),
    (204, 'Monitor', 12000),
    (205, 'Keyboard', 2000),
    (206, 'Mouse', 1500),
    (207, 'Speaker', 4000),
    (208, 'Printer', 7000),
    (209, 'Webcam', 2500),
    (210, 'Desk Lamp', 1800)
]

products_df = spark.createDataFrame(products_data, ["product_id", "product_name", "product_prices"])


#Organisation Dataframe

organization_data = [
    (1, 'Alice', 'Manager'),
    (2, 'Bob', 'Developer'),
    (3, 'Charlie', 'Analyst'),
    (4, 'David', 'Developer'),
    (5, 'Eva', 'HR'),
    (6, 'Frank', 'Manager'),
    (7, 'Grace', 'Designer'),
    (8, 'Heidi', 'Analyst')
]

organization_df = spark.createDataFrame(organization_data, ["emp_id", "emp_name", "jobs"])

#Order Dataframe

orders_data = [
    (1001, 1, '2024-12-01'),
    (1002, 3, '2024-12-03'),
    (1003, 2, '2024-12-05'),
    (1004, 5, '2024-12-06'),
    (1005, 4, '2024-12-07'),
    (1006, 6, '2024-12-08')
]

orders_df = spark.createDataFrame(orders_data, ["order_id", "customer_id", "order_date"])

#user Dataframe

users_data = [
    (1, 'John', 'Doe'),
    (2, 'Jane', 'Smith'),
    (3, 'Alice', 'Brown'),
    (4, 'Bob', 'Johnson'),
    (5, 'Alice', 'Brown'),  # duplicate
    (6, 'Tom', 'Hardy')
]

users_df = spark.createDataFrame(users_data, ["user_id", "first_name", "last_name"])


from pyspark.sql.functions import col

emp = employees_df.orderBy(col("salary").desc()).limit(5)
emp.show()

cus = customers_df.orderBy(col("name").asc())
cus.show()

sal = sales_df.orderBy(col("quantity_sold").desc()).limit(3)
sal.show()

pro = products_df.orderBy(col("product_prices").asc()).limit(10)
pro.show()

empdis = employees_df.select("department").distinct()
empdis.show()

ccus = customers_df.select("city").distinct().count()
print("Number of distinct cities:", ccus)

ord = orders_df.orderBy(col("order_date").asc()).limit(5)
ord.show()

job = organization_df.select(col("jobs")).distinct()
job.show()

com = users_df.select(col("first_name"),col("last_name")).distinct()
com.show()

sor = employees_df.orderBy(col("department").asc(), col("salary").desc())
sor.show()

