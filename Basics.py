# -*- coding: utf-8 -*-
"""Copy of Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tHWKWwbjnJhB4HhjIzA26m2InOkQsxmz
"""

!wget -q https://github.com/saiadityaus1/test1/raw/main/df.csv -O df.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/df1.csv -O df1.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/dt.txt -O dt.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file1.txt -O file1.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file2.txt -O file2.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file3.txt -O file3.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/file4.json -O file4.json
!wget -q https://github.com/saiadityaus1/test1/raw/main/file5.parquet -O file5.parquet
!wget -q https://github.com/saiadityaus1/test1/raw/main/file6 -O file6
!wget -q https://github.com/saiadityaus1/test1/raw/main/prod.csv -O prod.csv
!wget -q https://github.com/saiadityaus1/test1/raw/main/state.txt -O state.txt
!wget -q https://github.com/saiadityaus1/test1/raw/main/usdata.csv -O usdata.csv
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar xf spark-3.5.0-bin-hadoop3.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"
import findspark
findspark.init()
import sys
import os
python_path = sys.executable
os.environ['PYSPARK_PYTHON'] = python_path
from pyspark import SparkConf
from pyspark import SparkContext
from pyspark.sql import SparkSession
conf = SparkConf().setAppName("pyspark").setMaster("local[*]").set("spark.driver.host","localhost").set("spark.driver.allowMultipleContexts","true")
sc = SparkContext(conf=conf)
spark = SparkSession.builder.config("spark.driver.allowMultipleContexts","true").getOrCreate()
print("DONE==== START YOUR WORKðŸ‘‡ ")

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("Practice").getOrCreate()

data = [
    (1, "Alice", "Sales", 30000),
    (2, "Bob", "HR", 40000),
    (3, "Charlie", "Sales", 50000),
    (4, "David", "Finance", 60000),
    (5, "Eve", "HR", 55000),
    (6, "Frank", "Admin", 45000),
    (7, "Grace", "Finance", 52000),
    (8, "Arun", "HR", 30000)
]

columns = ["id", "name", "department", "salary"]

df = spark.createDataFrame(data, columns)
df.show()


seldf = df.select("id", "name", "department", "salary")
seldf.show()

fildf = df.filter("department ='Sales'")
fildf.show()

sindf = df.select("name")
sindf.show()

muldf = df.select("name", "salary")
muldf.show()

mulfil = df.filter("department = 'HR' and salary > 45000")
mulfil.show()

alpdf = df.filter("name like 'A%'")
alpdf.show()

negdf = df.filter ("department != 'Finance'")
negdf.show()

logdf = df.filter("salary > 40000 and department = 'Sales'")
logdf.show()

aldf = df.select(df.name.alias("employee_name"), df.salary.alias("emp_salary") )
aldf.show()

casedf = df.filter("name = 'Alice'")
casedf.show()

